{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cad876bc",
   "metadata": {},
   "source": [
    "### Notebook to train and test Mask R-CNN on your own dataset.\n",
    "\n",
    "##### All file paths need to be changed to direct to your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3253cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### install dependencies:\n",
    "\n",
    "# might need to install opencv\n",
    "!pip3 install Cython cupy-cuda112 cupy-cuda102   # Probably only need one of these but it works so I'm gonna leave it for now.\n",
    "!pip3 install pyyaml==5.1\n",
    "\n",
    "# Torch 1.8.1 does not work despite being the latest stable release. As such, use 1.7. \n",
    "# Cuda version on MAGEOHub is currently 11.3 - cu110 all works fine.\n",
    "\n",
    "# To be tried once the bug with torch 1.8 has been fixed \n",
    "#!pip3 install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!python -m pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu110/torch1.7/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce82d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import torch and check versions and cuda availability, and resources\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "!gcc --version\n",
    "!nvcc --version\n",
    "!nvidia-smi\n",
    "\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01c940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic imports:\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# tensorboard?\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir output\n",
    "\n",
    "# necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "#from google.colab.patches import cv2_imshow\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.structures import BoxMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280cb0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The following two cells just determine that the setup is working correctly on a standard example.\n",
    "\n",
    "# Get image, hash wget line if already downloaded.\n",
    "# Just unhash if running this code for the first time, i.e. input.jpg has not yet been downloaded\n",
    "\n",
    "!wget http://images.cocodataset.org/val2017/000000439715.jpg -O input.jpg\n",
    "im = cv2.imread(\"./input.jpg\")\n",
    "\n",
    "# Create config\n",
    "cfg = get_cfg()\n",
    "\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"))\n",
    "\n",
    "#cfg.merge_file = 'https://github.com/facebookresearch/detectron2/blob/master/configs/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml'\n",
    "#cfg.merge_from_file(\"./detectron2/configs/COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\")\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "\n",
    "# Bit worried this line won't work for everyone - if it doesn't, go to https://github.com/facebookresearch/detectron2/blob/master/MODEL_ZOO.md to find a link to the model weights\n",
    "\n",
    "#cfg.MODEL.WEIGHTS = \"detectron2://COCO-Detection/faster_rcnn_R_101_FPN_3x/137851257/model_final_f6e8b1.pkl\"\n",
    "\n",
    "# Here we just get the pre-trained weights straight from facebook hosting website\n",
    "\n",
    "cfg.MODEL.WEIGHTS = 'https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x/138205316/model_final_a3ec72.pkl'\n",
    "\n",
    "\n",
    "\n",
    "# Create predictor\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# Make prediction\n",
    "outputs = predictor(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1d40ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look\n",
    "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "\n",
    "image = cv2.cvtColor(v.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB)\n",
    "display(Image.fromarray(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d63b823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the inevitable error messages are useful.\n",
    "CUDA_LAUNCH_BLOCKING=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b7acb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This loop creates training data from pngs and GeoJSON files of manually delineated crowns\n",
    "\n",
    "def get_tree_dicts(directory):\n",
    "    classes = ['tree']\n",
    "    dataset_dicts = []\n",
    "    for filename in [file for file in os.listdir(directory) if file.endswith('.geojson')]:\n",
    "        json_file = os.path.join(directory, filename)\n",
    "        with open(json_file) as f:\n",
    "            img_anns = json.load(f)\n",
    "        \n",
    "        \n",
    "        \n",
    "        record = {}\n",
    "        \n",
    "        filename = os.path.join(directory, img_anns[\"imagePath\"])\n",
    "        # Make sure we have the correct height and width\n",
    "        height, width = cv2.imread(filename).shape[:2]\n",
    "\n",
    "        record[\"file_name\"] = filename\n",
    "        record[\"height\"] = height\n",
    "        record[\"width\"] = width\n",
    "        record[\"image_id\"] = filename[0:400]\n",
    "        print(filename[0:400])\n",
    "            \n",
    "        \n",
    "\n",
    "        objs = []\n",
    "        for features in img_anns['features']:\n",
    "            anno = features['geometry']\n",
    "            # print(\"##### HERE IS AN ANNO #####\", anno)...weirdly sometimes (but not always) have to make 1000 into a np.array\n",
    "            px = [a[0] for a in anno['coordinates'][0]]\n",
    "            py = [np.array(height) - a[1] for a in anno['coordinates'][0]]\n",
    "            # print(\"### HERE IS PY ###\", py)\n",
    "            poly = [(x, y) for x, y in zip(px, py)]\n",
    "            poly = [p for x in poly for p in x]\n",
    "            # print(\"#### HERE ARE SOME POLYS #####\", poly)\n",
    "            obj = {\n",
    "                   \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
    "                   \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                   \"segmentation\": [poly],\n",
    "                   \"category_id\": 0,\n",
    "                   \"iscrowd\": 0\n",
    "                   }\n",
    "            objs.append(obj)\n",
    "            # print(\"#### HERE IS OBJS #####\", objs)\n",
    "        record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return dataset_dicts\n",
    "\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "for d in [\"train\", \"test\"]:\n",
    "    DatasetCatalog.register(\"trees_\" + d, lambda d=d: get_tree_dicts('/home/jovyan/lustre_scratch/paracou_data/high_integrity_data/' + d))\n",
    "    MetadataCatalog.get(\"trees_\" + d).set(thing_classes=['tree'])\n",
    "trees_metadata = MetadataCatalog.get(\"trees_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaf94c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at our training images and annotations for our dataset!\n",
    "\n",
    "dataset_dicts = get_tree_dicts(\"/home/jovyan/lustre_scratch/paracou_data/high_integrity_data/train\")\n",
    "for d in random.sample(dataset_dicts, 15):\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=trees_metadata, scale=0.4)\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "    image = cv2.cvtColor(out.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB)\n",
    "    #display(Image.fromarray(img))\n",
    "    display(Image.fromarray(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70913c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup the evaluator\n",
    "\n",
    "from detectron2.evaluation import (\n",
    "    CityscapesInstanceEvaluator,\n",
    "    CityscapesSemSegEvaluator,\n",
    "    COCOEvaluator,\n",
    "    COCOPanopticEvaluator,\n",
    "    DatasetEvaluators,\n",
    "    LVISEvaluator,\n",
    "    PascalVOCDetectionEvaluator,\n",
    "    SemSegEvaluator,\n",
    "    verify_results,\n",
    ")\n",
    "\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a8228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further setting up the evaluator.\n",
    "\n",
    "from detectron2.engine.hooks import HookBase\n",
    "from detectron2.evaluation import inference_context\n",
    "from detectron2.utils.logger import log_every_n_seconds\n",
    "from detectron2.data import DatasetMapper, build_detection_test_loader, build_detection_train_loader\n",
    "import detectron2.utils.comm as comm\n",
    "#import torch\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "class LossEvalHook(HookBase):\n",
    "    def __init__(self, eval_period, model, data_loader):\n",
    "        self._model = model\n",
    "        self._period = eval_period\n",
    "        self._data_loader = data_loader\n",
    "    \n",
    "    def _do_loss_eval(self):\n",
    "        # Copying inference_on_dataset from evaluator.py\n",
    "        total = len(self._data_loader)\n",
    "        num_warmup = min(5, total - 1)\n",
    "            \n",
    "        start_time = time.perf_counter()\n",
    "        total_compute_time = 0\n",
    "        losses = []\n",
    "        for idx, inputs in enumerate(self._data_loader):            \n",
    "            if idx == num_warmup:\n",
    "                start_time = time.perf_counter()\n",
    "                total_compute_time = 0\n",
    "            start_compute_time = time.perf_counter()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            total_compute_time += time.perf_counter() - start_compute_time\n",
    "            iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)\n",
    "            seconds_per_img = total_compute_time / iters_after_start\n",
    "            if idx >= num_warmup * 2 or seconds_per_img > 5:\n",
    "                total_seconds_per_img = (time.perf_counter() - start_time) / iters_after_start\n",
    "                eta = datetime.timedelta(seconds=int(total_seconds_per_img * (total - idx - 1)))\n",
    "                log_every_n_seconds(\n",
    "                    logging.INFO,\n",
    "                    \"Loss on Validation  done {}/{}. {:.4f} s / img. ETA={}\".format(\n",
    "                        idx + 1, total, seconds_per_img, str(eta)\n",
    "                    ),\n",
    "                    n=5,\n",
    "                )\n",
    "            loss_batch = self._get_loss(inputs)\n",
    "            losses.append(loss_batch)\n",
    "        mean_loss = np.mean(losses)\n",
    "        self.trainer.storage.put_scalar('validation_loss', mean_loss)\n",
    "        comm.synchronize()\n",
    "\n",
    "        return losses\n",
    "            \n",
    "    def _get_loss(self, data):\n",
    "        # How loss is calculated on train_loop \n",
    "        metrics_dict = self._model(data)\n",
    "        metrics_dict = {\n",
    "            k: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)\n",
    "            for k, v in metrics_dict.items()\n",
    "        }\n",
    "        total_losses_reduced = sum(loss for loss in metrics_dict.values())\n",
    "        return total_losses_reduced\n",
    "        \n",
    "        \n",
    "    def after_step(self):\n",
    "        next_iter = self.trainer.iter + 1\n",
    "        is_final = next_iter == self.trainer.max_iter\n",
    "        if is_final or (self._period > 0 and next_iter % self._period == 0):\n",
    "            self._do_loss_eval()\n",
    "        self.trainer.storage.put_scalars(timetest=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8945cedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the config file.\n",
    "\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"trees_train\",)\n",
    "cfg.DATASETS.TEST = (\"trees_test\",)\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\")\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025\n",
    "cfg.SOLVER.MAX_ITER = 2000\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3661d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom trainer, which will also print evaluations.\n",
    "\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "import detectron2.data.transforms as T\n",
    "from detectron2.data import detection_utils as utils\n",
    "\n",
    "class MyTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            os.makedirs(\"eval_2\", exist_ok=True)\n",
    "            output_folder = \"eval_2\"\n",
    "        return COCOEvaluator(dataset_name, cfg, True, output_folder)\n",
    "                     \n",
    "    def build_hooks(self):\n",
    "        hooks = super().build_hooks()\n",
    "        hooks.insert(-1,LossEvalHook(\n",
    "            cfg.TEST.EVAL_PERIOD,\n",
    "            self.model,\n",
    "            build_detection_test_loader(\n",
    "                self.cfg,\n",
    "                self.cfg.DATASETS.TEST[0],\n",
    "                DatasetMapper(self.cfg,True)\n",
    "            )\n",
    "        ))\n",
    "        return hooks\n",
    "\n",
    "    def build_train_loader(cls, cfg):\n",
    "        return build_detection_train_loader(cfg, mapper=DatasetMapper(cfg, is_train=True, augmentations=[T.Resize((800, 800)), T.RandomBrightness(0.8, 1.8), \n",
    "        T.RandomContrast(0.6, 1.3), T.RandomSaturation(0.8, 1.4), T.RandomRotation(angle=[90, 90], expand=False), T.RandomLighting(0.7),\n",
    "        T.RandomFlip(prob=0.4, horizontal=False, vertical=True)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1233de61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train! Can change hyperparameters e.g. learning rate.\n",
    "# 2000 iterations will take approximately 20 minutes using a single GPU.\n",
    "\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"trees_train\",)\n",
    "cfg.DATASETS.TEST = (\"trees_test\",)\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\")\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025\n",
    "cfg.SOLVER.MAX_ITER = 2000\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "\n",
    "### From here is the important bit that hasn't been repeated further up\n",
    "cfg.TEST.EVAL_PERIOD = 10\n",
    "\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = MyTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "#eval_results = inference_on_dataset(trainer.model, val_loader, DatasetEvaluators(evaluator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1bfb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot training and validation loss on the same plot to see the ideal number of iterations of training.\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "experiment_folder = '/home/jovyan/detectron2_live/train_test/output'\n",
    "\n",
    "def load_json_arr(json_path):\n",
    "    lines = []\n",
    "    with open(json_path, 'r') as f:\n",
    "        for line in f:\n",
    "            lines.append(json.loads(line))\n",
    "    return lines\n",
    "\n",
    "experiment_metrics = load_json_arr(experiment_folder + '/metrics.json')\n",
    "\n",
    "plt.plot(\n",
    "    [x['iteration'] for x in experiment_metrics if 'validation_loss' in x], \n",
    "    [x['validation_loss'] for x in experiment_metrics if 'validation_loss' in x], label='Total Validation Loss', color='red')\n",
    "plt.plot(\n",
    "    [x['iteration'] for x in experiment_metrics if 'total_loss' in x], \n",
    "    [x['total_loss'] for x in experiment_metrics if 'total_loss' in x], label='Total Training Loss')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Comparison of the training and validation loss of Mask R-CNN')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b6f4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup to predict on new images, here setting up for the trees_test dataset, but can also use this setup\n",
    "# for predicting on individual images as seen 2 cells down\n",
    "\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "\n",
    "# Weights automatically saved to OUTPUT_DIR + model_final.pth following training\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.1\n",
    "cfg.DATASETS.TEST = (\"trees_test\",)\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2530a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at our prediction on our test dataset.\n",
    "\n",
    "dataset_dicts = get_tree_dicts(\"/home/jovyan/lustre_scratch/paracou_data/high_integrity_data/test\")\n",
    "for d in random.sample(dataset_dicts, 3):\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(img)\n",
    "    v = Visualizer(img[:, :, ::-1], metadata=trees_metadata, scale=0.7, instance_mode=ColorMode.IMAGE_BW)   # remove the colors of unsegmented pixels\n",
    "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    image = cv2.cvtColor(v.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB)\n",
    "    display(Image.fromarray(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349a668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make and visualise predictions on a particular image\n",
    "\n",
    "im = cv2.imread(\"/home/jovyan/lustre_scratch/sepilok_data/all_sepilok_2014_rgb_tiles_pngs/tile_602400_646900.png\")\n",
    "outputs = predictor(im)\n",
    "v = Visualizer(im[:, :, ::-1], metadata=trees_metadata, scale=1.5, instance_mode=ColorMode.IMAGE_BW)   # remove the colors of unsegmented pixels\n",
    "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "image = cv2.cvtColor(v.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB)\n",
    "display(Image.fromarray(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b16f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Need these packages\n",
    "\n",
    "!pip install Fiona\n",
    "!pip install rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a596af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.transform import from_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295c6b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading in dataset dicts for all our areas of interest\n",
    "\n",
    "dataset_dicts = get_tree_dicts(\"/home/jovyan/lustre_scratch/paracou_data/high_integrity_data/test\")\n",
    "for d in dataset_dicts:\n",
    "    print(d[\"file_name\"][-10:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6e9294",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filepath in glob.iglob('/home/jovyan/lustre_scratch/sepilok_data/all_sepilok_2014_rgb_tiles_pngs/*.png'):\n",
    "    #print(filepath)\n",
    "    print(filepath[-17:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98aa22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell outputs Mask R-CNN's predictions as individual rasters\n",
    "\n",
    "import glob\n",
    "\n",
    "for filepath in glob.iglob('/home/jovyan/lustre_scratch/sepilok_data/all_sepilok_2014_rgb_tiles_pngs/*.png'):\n",
    "    print(filepath)\n",
    "    img = cv2.imread(filepath)\n",
    "    outputs = predictor(img)\n",
    "    mask_array = outputs['instances'].pred_masks.cpu().numpy()\n",
    "    \n",
    "    \n",
    "    #print(mask_array.shape)\n",
    "    # Shifting axes...not sure why...to investigate\n",
    "    #mask_array = np.moveaxis(mask_array, 0, -1)\n",
    "    #print(mask_array.shape)\n",
    "    # here are the shapes for all the different predictions on all the different images\n",
    "    #print(mask_array.shape)\n",
    "\n",
    "    num_instances = mask_array.shape[0]\n",
    "    #print(num_instances)\n",
    "    mask_array_instance = []\n",
    "    # black (think this putting zeroes where there are zeroes in our image??? NEED TO EXAMINE)\n",
    "    output = np.zeros_like(mask_array) \n",
    "    #print('output',output.shape)\n",
    "\n",
    "    mask_array_instance.append(mask_array)\n",
    "    output = np.where(mask_array_instance[0] == True, 255, output)\n",
    "    #print(output)\n",
    "    #print(mask_array_instance)\n",
    "    fresh_output = output.astype(np.float)\n",
    "    #print(fresh_output.shape)\n",
    "    x_scaling = 200/fresh_output.shape[1]\n",
    "    y_scaling = 200/fresh_output.shape[2]\n",
    "    # this is an affine transform. This needs to be altered significantly.\n",
    "    transform = from_origin(int(filepath[-17:-11]), int(filepath[-10:-4])+200, y_scaling, x_scaling)\n",
    "\n",
    "    new_dataset = rasterio.open('/home/jovyan/lustre_scratch/sepilok_data/all_sepilok_predictions/10_predicted_polygons_'+filepath[-17:-4]+'.tif', 'w', driver='GTiff',\n",
    "                                height = fresh_output.shape[1], width = fresh_output.shape[2], count = fresh_output.shape[0],\n",
    "                                dtype=str(fresh_output.dtype),\n",
    "                                crs='+proj=utm +zone=50 +datum=WGS84 +units=m +no_defs',  \n",
    "                                transform=transform)\n",
    "\n",
    "    new_dataset.write(fresh_output)\n",
    "    new_dataset.close()\n",
    "\n",
    "### Working fantastically, 30/6/21.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339fde7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This cell outputs large rasters WITH THE NUMBER OF LAYERS = NUMBER OF PREDS\n",
    "### stacks the raster predictions\n",
    "### this creates rasters of shape (number of preds, height, width)\n",
    "\n",
    "dataset_dicts = get_tree_dicts(\"/home/jovyan/lustre_scratch/paracou_data/high_integrity_data/test\")\n",
    "for d in dataset_dicts:\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(img)\n",
    "    mask_array = outputs['instances'].pred_masks.cpu().numpy()\n",
    "    \n",
    "    \n",
    "    #print(mask_array.shape)\n",
    "    # Shifting axes...not sure why...to investigate\n",
    "    #mask_array = np.moveaxis(mask_array, 0, -1)\n",
    "    #print(mask_array.shape)\n",
    "    # here are the shapes for all the different predictions on all the different images\n",
    "    #print(mask_array.shape)\n",
    "\n",
    "    num_instances = mask_array.shape[0]\n",
    "    #print(num_instances)\n",
    "    mask_array_instance = []\n",
    "    # black (think this putting zeroes where there are zeroes in our image??? NEED TO EXAMINE)\n",
    "    output = np.zeros_like(mask_array) \n",
    "    #print('output',output.shape)\n",
    "\n",
    "    mask_array_instance.append(mask_array)\n",
    "    output = np.where(mask_array_instance[0] == True, 255, output)\n",
    "    #print(output)\n",
    "    #print(mask_array_instance)\n",
    "    fresh_output = output.astype(np.float)\n",
    "    #print(fresh_output.shape)\n",
    "    x_scaling = 100/fresh_output.shape[1]\n",
    "    y_scaling = 100/fresh_output.shape[2]\n",
    "    # this is an affine transform. This needs to be altered significantly.\n",
    "    transform = from_origin(int(d[\"file_name\"][-17:-11]), int(d[\"file_name\"][-10:-4])+100, y_scaling, x_scaling)\n",
    "\n",
    "    new_dataset = rasterio.open('10_predicted_polygons_'+d[\"file_name\"][-32:-4]+'.tif', 'w', driver='GTiff',\n",
    "                                height = fresh_output.shape[1], width = fresh_output.shape[2], count = fresh_output.shape[0],\n",
    "                                dtype=str(fresh_output.dtype),\n",
    "                                crs='+proj=utm +zone=22 +datum=WGS84 +units=m +no_defs',  \n",
    "                                transform=transform)\n",
    "\n",
    "    new_dataset.write(fresh_output)\n",
    "    new_dataset.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d091aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### It reads in a stack raster of all the predictions, and outputs n shapefiles, where n \n",
    "### is the number of predictions for that particular image. \n",
    "### We can then merge all these predictions. \n",
    "\n",
    "import numpy as np\n",
    "import fiona\n",
    "import rasterio\n",
    "import rasterio.features\n",
    "from shapely.geometry import shape, mapping\n",
    "from shapely.geometry.multipolygon import MultiPolygon\n",
    "#from shapely.geometry import Polygon\n",
    "\n",
    "\n",
    "# Read input band with Rasterio\n",
    "for filepath in glob.iglob('/home/jovyan/lustre_scratch/sepilok_data/all_sepilok_predictions/*.tif'):\n",
    "    print(filepath)\n",
    "    with rasterio.open(filepath) as src:\n",
    "        shp_schema = {'geometry': 'MultiPolygon','properties': {'pixelvalue': 'int'}}\n",
    "    \n",
    "        crs = src.crs\n",
    "        for i in range(src.count):\n",
    "            src_band = src.read(i+1)\n",
    "            src_band = np.float32(src_band)\n",
    "            #print(src_band.dtype)\n",
    "            # Keep track of unique pixel values in the input band\n",
    "            unique_values = np.unique(src_band)\n",
    "            # Polygonize with Rasterio. `shapes()` returns an iterable\n",
    "            # of (geom, value) as tuples\n",
    "            shapes = list(rasterio.features.shapes(src_band, transform=src.transform))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get a list of all polygons for a given pixel value\n",
    "# and create a MultiPolygon geometry with shapely.\n",
    "# Then write the record to an output shapefile with fiona.\n",
    "# We make use of the `shape()` and `mapping()` functions from\n",
    "# shapely to translate between the GeoJSON-like dict format\n",
    "# and the shapely geometry type.\n",
    "##### Don't forget to add this folder here!\n",
    "            with fiona.open('/home/jovyan/lustre_scratch/sepilok_data/all_sepilok_preds_shapes/10_predicted_polygons_'+filepath[-17:-4]+'_'+str(i)+'.shp', 'w', 'ESRI Shapefile', shp_schema, crs) as shp:\n",
    "                for pixel_value in unique_values:\n",
    "                    polygons = [shape(geom) for geom, value in shapes\n",
    "                                if value == pixel_value]\n",
    "                    multipolygon = MultiPolygon(polygons)\n",
    "                    shp.write({\n",
    "                        'geometry': mapping(multipolygon),\n",
    "                        'properties': {'pixelvalue': int(pixel_value)}\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fdbc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile as zf\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c42128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### zip up the folder to download\n",
    "\n",
    "shutil.make_archive(\"/home/jovyan/lustre_scratch/sepilok_data/all_sepilok_preds_shapes\", \"zip\", \"/home/jovyan/lustre_scratch/sepilok_data/all_sepilok_preds_shapes\")\n",
    "\n",
    "#zip -r /home/jovyan/lustre_scratch/sepilok_data/all_sepilok_preds_shapes.zip /home/jovyan/lustre_scratch/sepilok_data/all_sepilok_preds_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933a311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EOF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
